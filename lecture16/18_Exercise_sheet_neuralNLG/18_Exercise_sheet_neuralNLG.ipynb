{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Lecture 16: Neural Generation\n",
    "\n",
    "In this assignement, we implement an RNN model which learns to generate a film title.\n",
    "\n",
    "A language model can predict the probability of the next word given a sequence of preceding words. Here we will create a character based language model to learn to generate a film title. \n",
    "\n",
    "The data is in the movies-sf.txt file on Arche. Each line of that line contains a film title followed by the year it came out (in brackets). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.1 - Creating char2int and int2char dictionaries\n",
    "\n",
    "* Create the dictionaries mapping tokens to integers and back. Here because we train a character based model the tokens are the characters contained in the corpus. Your dictionary should include two special symbols:\n",
    "\n",
    "- the `<eos>` symbol for padding\n",
    "- the `<start>` symbol which indicates the start of the sequence (null context, this will be used as input when generating)\n",
    "\n",
    "* Use your dictionary to convert each film titles in the corpus to sequences of integers (store this in a variable; it will be used in Exercise 2.1).\n",
    "\n",
    "**Hint :** Cf. Exercise 1.2 in Exercise sheet 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\experiments\\cours nlp\\data science\\lecture16\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"movies-sf.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Passengers (2016'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0][:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "token2int = defaultdict(lambda: len(token2int))\n",
    "token2int['<eos>'] = 0\n",
    "token2int['<start>'] = 1\n",
    "for l in text:\n",
    "    [token2int[token.lower()] for token in l[:-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2token = { k : v for v, k in token2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_int = [[1]+[token2int[token.lower()] for token in l[:-2]] for l in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2 - Print out the number of titles in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tittle in the data: 7205\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of tittle in the data: {len(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.3 - Print out the histogram of title lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the histogram of title lengths to decide what the maximum length will be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAATnElEQVR4nO3dbaxd1X3n8e9vIJCEdjAPdyzXtsaMYiVC0UDIFXWUqEqh6fAQxXmRMKRVcZElzwtmmjSVijsjTRppRjJSVUrUEZIV0poqJaE0GVsEpWUMUdWRIL0mhAAO4oaY2BbgGwpkGpQmtP95cZbLwdi+5z4fL38/0tFZe+29z/lfn3N/3nedffZKVSFJ6su/WukCJEmLz3CXpA4Z7pLUIcNdkjpkuEtSh85c6QIALrzwwtqwYcNKlyFJp5R9+/b9sKomjrduLMJ9w4YNTE1NrXQZknRKSfLsidY5LCNJHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0ai2+onio2bP/aCdcd2HHtMlYiSSfnkbskdchwl6QOGe6S1CHDXZI6ZLhLUoc8W2aReCaNpHHikbskdchwl6QOGe6S1KGRwj3Jbyd5IsnjSe5K8tYkFyV5OMl0ki8nOatte3Zbnm7rNyzpTyBJepNZwz3JWuC3gMmqejdwBnA9cAtwa1W9A3gJ2Np22Qq81PpvbdtJkpbRqMMyZwJvS3Im8HbgOeAK4J62fhfw0dbe3JZp669MkkWpVpI0klnDvaoOA38A/IBBqL8C7ANerqrX2maHgLWtvRY42PZ9rW1/wbGPm2RbkqkkUzMzMwv9OSRJQ0YZljmPwdH4RcAvAOcAVy30iatqZ1VNVtXkxMTEQh9OkjRklGGZXwG+X1UzVfUz4CvA+4FVbZgGYB1wuLUPA+sB2vpzgRcXtWpJ0kmNEu4/ADYleXsbO78SeBJ4EPhY22YLsLu197Rl2voHqqoWr2RJ0mxGGXN/mMEHo48A32n77ARuBj6dZJrBmPodbZc7gAta/6eB7UtQtyTpJEa6tkxVfQb4zDHdzwCXH2fbnwAfX3hpkqT58huqktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjo00uUHtHQ2bP/acfsP7Lh2mSuR1BOP3CWpQ4a7JHXIcJekDhnuktShUeZQfWeSR4duP0ryqSTnJ7k/ydPt/ry2fZJ8Lsl0kseSXLb0P4YkadgoMzE9VVWXVtWlwHuBV4GvMphhaW9VbQT28vqMS1cDG9ttG3D7EtQtSTqJuQ7LXAl8r6qeBTYDu1r/LuCjrb0ZuLMGHmIwkfaaxShWkjSauYb79cBdrb26qp5r7eeB1a29Fjg4tM+h1vcGSbYlmUoyNTMzM8cyJEknM3K4JzkL+AjwF8euq6oCai5PXFU7q2qyqiYnJibmsqskaRZzOXK/Gnikql5oyy8cHW5p90da/2Fg/dB+61qfJGmZzCXcP8HrQzIAe4Atrb0F2D3Uf0M7a2YT8MrQ8I0kaRmMdG2ZJOcAHwL+01D3DuDuJFuBZ4HrWv99wDXANIMza25ctGolSSMZKdyr6sfABcf0vcjg7Jljty3gpkWpTpI0L35DVZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA6NFO5JViW5J8l3k+xP8r4k5ye5P8nT7f68tm2SfC7JdJLHkly2tD+CJOlYox653wZ8vareBVwC7Ae2A3uraiOwty3DYK7Vje22Dbh9USuWJM1q1pmYkpwL/BLwmwBV9VPgp0k2Ax9sm+0CvgHcDGwG7mwzMj3UjvrXOI/q3GzY/rUTrjuw49plrETSqWiUI/eLgBngT5J8K8nn25yqq4cC+3lgdWuvBQ4O7X+o9UmSlsko4X4mcBlwe1W9B/gxrw/BAP8yb2rN5YmTbEsylWRqZmZmLrtKkmYxSrgfAg5V1cNt+R4GYf9CkjUA7f5IW38YWD+0/7rW9wZVtbOqJqtqcmJiYr71S5KOY9Zwr6rngYNJ3tm6rgSeBPYAW1rfFmB3a+8BbmhnzWwCXnG8XZKW16wfqDb/BfhikrOAZ4AbGfzHcHeSrcCzwHVt2/uAa4Bp4NW2rSRpGY0U7lX1KDB5nFVXHmfbAm5aWFmSpIXwG6qS1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA6NFO5JDiT5TpJHk0y1vvOT3J/k6XZ/XutPks8lmU7yWJLLlvIHkCS92ajT7AH8clX9cGh5O7C3qnYk2d6WbwauBja22y8Ct7d7LYMN2792wnUHdly7jJVIWkkLGZbZDOxq7V3AR4f676yBh4BVSdYs4HkkSXM0argX8NdJ9iXZ1vpWV9Vzrf08sLq11wIHh/Y91PreIMm2JFNJpmZmZuZRuiTpREYdlvlAVR1O8m+A+5N8d3hlVVWSmssTV9VOYCfA5OTknPaVJJ3cSEfuVXW43R8BvgpcDrxwdLil3R9pmx8G1g/tvq71SZKWyazhnuScJD9/tA38KvA4sAfY0jbbAuxu7T3ADe2smU3AK0PDN5KkZTDKsMxq4KtJjm7/51X19SR/B9ydZCvwLHBd2/4+4BpgGngVuHHRq5YkndSs4V5VzwCXHKf/ReDK4/QXcNOiVCdJmhe/oSpJHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktShuUyQrXk62aTVkrQUPHKXpA6NfOSe5AxgCjhcVR9OchHwJeACYB/wG1X10yRnA3cC7wVeBP5jVR1Y9Mo1Zyf6C+LAjmuXuRJJS20uR+6fBPYPLd8C3FpV7wBeAra2/q3AS63/1radJGkZjXTknmQdcC3wP4FPZzDn3hXAr7VNdgG/D9wObG5tgHuAP06SNkPT2DjZOLhHspJOdaMeuf8R8LvAP7flC4CXq+q1tnwIWNvaa4GDAG39K237N0iyLclUkqmZmZn5VS9JOq5Zwz3Jh4EjVbVvMZ+4qnZW1WRVTU5MTCzmQ0vSaW+UYZn3Ax9Jcg3wVuBfA7cBq5Kc2Y7O1wGH2/aHgfXAoSRnAucy+GBVkrRMZj1yr6rfq6p1VbUBuB54oKp+HXgQ+FjbbAuwu7X3tGXa+gfGbbxdknq3kPPcb2bw4eo0gzH1O1r/HcAFrf/TwPaFlShJmqs5fUO1qr4BfKO1nwEuP842PwE+vgi1SZLmyW+oSlKHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIWdiOg5nTpJ0qvPIXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDo0yh+pbk3wzybeTPJHks63/oiQPJ5lO8uUkZ7X+s9vydFu/YYl/BknSMUY5cv9H4IqqugS4FLgqySbgFuDWqnoH8BKwtW2/FXip9d/atpMkLaNZv6Ha5j/9h7b4lnYr4Arg11r/LuD3gduBza0NcA/wx0niPKqnppN9W/fAjmuXsRJJczHSmHuSM5I8ChwB7ge+B7xcVa+1TQ4Ba1t7LXAQoK1/hcEcq8c+5rYkU0mmZmZmFvRDSJLeaKRwr6p/qqpLgXUM5k1910KfuKp2VtVkVU1OTEws9OEkSUPmdLZMVb0MPAi8D1iV5OiwzjrgcGsfBtYDtPXnAi8uRrGSpNGMcrbMRJJVrf024EPAfgYh/7G22RZgd2vvacu09Q843i5Jy2uUS/6uAXYlOYPBfwZ3V9W9SZ4EvpTkfwDfAu5o298B/FmSaeDvgeuXoG5J0kmMcrbMY8B7jtP/DIPx92P7fwJ8fFGqkyTNi99QlaQOORPTKciZoiTNxiN3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIC4dp3pw8Wxpfo8zEtD7Jg0meTPJEkk+2/vOT3J/k6XZ/XutPks8lmU7yWJLLlvqHkCS90SjDMq8Bv1NVFwObgJuSXAxsB/ZW1UZgb1sGuBrY2G7bgNsXvWpJ0kmNMhPTc8Bzrf3/kuwH1gKbgQ+2zXYB3wBubv13tnlTH0qyKsma9jg6TThkI62sOX2gmmQDgyn3HgZWDwX288Dq1l4LHBza7VDrO/axtiWZSjI1MzMz17olSScxcrgn+TngL4FPVdWPhte1o/SayxNX1c6qmqyqyYmJibnsKkmaxUjhnuQtDIL9i1X1ldb9QpI1bf0a4EjrPwysH9p9XeuTJC2TUc6WCXAHsL+q/nBo1R5gS2tvAXYP9d/QzprZBLzieLskLa9RznN/P/AbwHeSPNr6/iuwA7g7yVbgWeC6tu4+4BpgGngVuHExC54LJ5KWdLoa5WyZvwVygtVXHmf7Am5aYF2SpAXw8gOS1CEvPyCHr6QOeeQuSR0y3CWpQ4a7JHXIcJekDhnuktQhz5bR2PBKktLi8chdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOzXqee5IvAB8GjlTVu1vf+cCXgQ3AAeC6qnqpzdp0G4PJOl4FfrOqHlma0nU68Rx4aW5GOXL/U+CqY/q2A3uraiOwty0DXA1sbLdtwO2LU6YkaS5mDfeq+hvg74/p3gzsau1dwEeH+u+sgYeAVUcn0ZYkLZ/5jrmvHpr0+nlgdWuvBQ4ObXeo9UmSltGCP1Btc6bWXPdLsi3JVJKpmZmZhZYhSRoy33B/4ehwS7s/0voPA+uHtlvX+t6kqnZW1WRVTU5MTMyzDEnS8cw33PcAW1p7C7B7qP+GDGwCXhkavpEkLZNRToW8C/ggcGGSQ8BngB3A3Um2As8C17XN72NwGuQ0g1Mhb1yCmiVJs5g13KvqEydYdeVxti3gpoUWJUlaGCfr0LI72ReSJC0OLz8gSR0y3CWpQw7LqGsnGgLyejTqnUfuktQhw12SOuSwjE5LXkJYvTPcdcrz1ErpzRyWkaQOGe6S1CHDXZI65Ji7NAd+EKtThUfuktShU/7I3TMlJOnNTvlwlxbbfA8Y5nOpg/k+l0NAmo3hLi2x5fzr0s8EdNSShHuSq4DbgDOAz1fVjqV4Hkmj86+E08uif6Ca5AzgfwFXAxcDn0hy8WI/jyTpxJbiyP1yYLqqngFI8iVgM/DkEjyXdFoal6Ge+VjszyD8TOP4liLc1wIHh5YPAb947EZJtgHb2uI/JHlqCWo5nguBHy7Tc82H9S2M9S3cktaYWxb8EG+obxEe700W+JjL+Rr/2xOtWLEPVKtqJ7BzuZ83yVRVTS73847K+hbG+hZu3Gu0vtEsxZeYDgPrh5bXtT5J0jJZinD/O2BjkouSnAVcD+xZgueRJJ3Aog/LVNVrSf4z8FcMToX8QlU9sdjPswDLPhQ0R9a3MNa3cONeo/WNIFW10jVIkhaZFw6TpA4Z7pLUoa7DPckXkhxJ8vhQ3/lJ7k/ydLs/bwXrW5/kwSRPJnkiySfHqcYkb03yzSTfbvV9tvVflOThJNNJvtw+OF8xSc5I8q0k945bfUkOJPlOkkeTTLW+sXh9Wy2rktyT5LtJ9id537jUl+Sd7d/t6O1HST41LvW1Gn+7/W48nuSu9jszFu+/rsMd+FPgqmP6tgN7q2ojsLctr5TXgN+pqouBTcBN7VIN41LjPwJXVNUlwKXAVUk2AbcAt1bVO4CXgK0rVN9RnwT2Dy2PW32/XFWXDp37PC6vLwyuAfX1qnoXcAmDf8exqK+qnmr/bpcC7wVeBb46LvUlWQv8FjBZVe9mcALJ9YzL+6+qur4BG4DHh5afAta09hrgqZWucai23cCHxrFG4O3AIwy+bfxD4MzW/z7gr1awrnUMfsGvAO4FMmb1HQAuPKZvLF5f4Fzg+7QTK8atvmNq+lXg/45Tfbz+bfzzGZx5eC/wH8bl/df7kfvxrK6q51r7eWD1ShZzVJINwHuAhxmjGtuQx6PAEeB+4HvAy1X1WtvkEIM3+Ur5I+B3gX9uyxcwXvUV8NdJ9rVLbsD4vL4XATPAn7Rhrc8nOWeM6ht2PXBXa49FfVV1GPgD4AfAc8ArwD7G5P13Oob7v6jBf60rfi5okp8D/hL4VFX9aHjdStdYVf9Ugz+L1zG4KNy7VqqWYyX5MHCkqvatdC0n8YGquozBVVJvSvJLwytX+PU9E7gMuL2q3gP8mGOGOFb6/QfQxqw/AvzFsetWsr421r+ZwX+SvwCcw5uHgVfM6RjuLyRZA9Duj6xkMUnewiDYv1hVX2ndY1UjQFW9DDzI4M/MVUmOfgFuJS8v8X7gI0kOAF9iMDRzG+NT39GjO6rqCIPx4ssZn9f3EHCoqh5uy/cwCPtxqe+oq4FHquqFtjwu9f0K8P2qmqmqnwFfYfCeHIv33+kY7nuALa29hcE494pIEuAOYH9V/eHQqrGoMclEklWt/TYGnwfsZxDyH1vp+qrq96pqXVVtYPBn+wNV9evjUl+Sc5L8/NE2g3HjxxmT17eqngcOJnln67qSwaW5x6K+IZ/g9SEZGJ/6fgBsSvL29rt89N9vLN5/K/ohyTJ84HEXg7GwnzE4StnKYEx2L/A08H+A81ewvg8w+JPyMeDRdrtmXGoE/j3wrVbf48B/b/3/DvgmMM3gT+Wzx+C1/iBw7zjV1+r4drs9Afy31j8Wr2+r5VJgqr3G/xs4b8zqOwd4ETh3qG+c6vss8N32+/FnwNnj8v7z8gOS1KHTcVhGkrpnuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QO/X9lJA56FKxiEAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist([len(l[:-2]) for l in text], bins=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most titles have a length under 40 characters so we'll use 40 as maximum length when encoding the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 40\n",
    "pad_char = []\n",
    "pred_char = []\n",
    "for char in char_int:\n",
    "    to_add = max_len - len(char)\n",
    "    if len(char) > 40:\n",
    "        pred_char.append(char[1:41])\n",
    "        char += [0]*to_add\n",
    "    else:\n",
    "        pred_char.append(char[1:]+[0])\n",
    "        char += [0]*to_add\n",
    "        pad_char.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the training and validation data\n",
    "\n",
    "The RNN will iterate over the characters in x and predict the next character. Hence x is the sequence of characters over which the RNN iterates and y is the same sequence shifted by one character e.g. \n",
    "\n",
    "x = [1, 31, 19, 5, 9, 2, 32, ...]   \n",
    "y = [31, 19, 5, 9, 2, 32, 6 ...]\n",
    "\n",
    "where each integer corresponds to a character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "max_len = 40\n",
    "batch_size = 8\n",
    "embed_size = 16\n",
    "hidden_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise  2.1 - Creating tensors\n",
    "\n",
    "To create the X and Y tensors\n",
    "\n",
    "* Create two long tensors X and Y of dimension (nb of training instances, max input length) and whose values are 0  (use torch.zeros())\n",
    "* Iterate over the integer version of the data file you created in Exercise 1.1 and for each line l with (integer converted) title t\n",
    "   - calculate the length of the title to be encoded (which is the length of the title if the title is shorter than the max len and otherwise the max len)\n",
    "   - store t into a LongTensor **up to the character before last** (i.e. omit the last character)\n",
    "   - set the l row of X to the long tensor version of t\n",
    "* This gives you the tensor for the input, X\n",
    "* To create Y do the same thing but for each title omit the first character and store the title up to and including the last character\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.zeros(7205, 40)\n",
    "Y = torch.zeros(7205, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7205):\n",
    "    for j in range(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise  2.2 - Checking the tensors\n",
    "\n",
    "Check that your X and Y tensors are ok by printing the same row for both (e.g., X[30] and Y[30].\n",
    "\n",
    "* Use the tolist() method \n",
    "* print both the integer and the character version (using the int2char dic created above)\n",
    "\n",
    "You should see 2 lists that are almost identical: Y is X shifted by one character to the right e.g., \n",
    "\n",
    "\n",
    "X[30] = [1, 31, 19, 5, 9, 2, 32, ...]   \n",
    "Y[30] = [31, 19, 5, 9, 2, 32, 6 ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercise 2.2  - Split X and Y into  train and validation data\n",
    "\n",
    "* Split X into two parts, one called X_train which consists of the first 6500 items and the other called X_valid which includes the rest of the data\n",
    "* Do the same for Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise  2.3 - Use torch DataLoader to split training and validation data into batches\n",
    "\n",
    "**Hint:** This was provided in the previous lab sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the Neural Network\n",
    "\n",
    "We use a GRU (Gated Recurrent Network) as a recurrent network. There are three layers -  one embedding layer which encodes the input character into a character embedding/representation, one GRU layer (which may itself have multiple layers) that operates on that character embedding and a hidden state, and a decoder (generation) layer that outputs the probability distribution.\n",
    "\n",
    "* Pytorch [linear](https://pytorch.org/docs/stable/nn.html?highlight=linear#torch.nn.Linear)\n",
    "\n",
    "* Pytorch [GRU](https://pytorch.org/docs/stable/nn.html?highlight=gru#torch.nn.GRU)\n",
    "\n",
    "* The main difference with the sequence tagging approach is that at each recurring step, the RNN now takes an additional input (hidden) which is the hidden state produced at the preceding time step. This ensures that the prediction made depends on the character that was generated by the RNN at the previous time step (and indirectly at all previous time steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.1 - Specify a language model\n",
    "\n",
    "* Modify the model provided in the previous exercise sheet (Exercise 4.1) so that the previous hidden state is input to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.2  - Checking the model output\n",
    "\n",
    "* Print out the dimensions of the two tensors (output, hidden) output by your model when applied to X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output is of size (nb of training instances, max input length, nb of labels)\n",
    "# Hidden is of size (nb of layers, nb of training instances, hidden size)\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Define a function to evaluate the performance of the model (PROVIDED)\n",
    "\n",
    "The function compute the perplexity of the model on the data: \n",
    "\n",
    "$\n",
    "PP(x) = P(x)^{-\\frac{1}{N}} = \\left[\\prod_i P(x_i)\\right]^{-\\frac{1}{N}}\n",
    "$\n",
    "\n",
    "where $x$ is a sequence of words, $P(x)=\\prod_i P(x_i)$ is the probability assigned to that sequence by the model and $N$ its lenght. This can be rewritten in the log domain as:\n",
    "\n",
    "$\n",
    "PP(x) = exp\\left(-\\frac{1}{N}\\sum_i \\log P(x_i)\\right)\n",
    "$\n",
    "\n",
    "Since the loss function returns $-\\frac{1}{N}\\log P(x_i)$, perplexity can be computed by taking the exponential of the avg loss. Note that we do not handle the padding hence perplexity will be affected by it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def perf(model, loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    total_loss = num = 0\n",
    "    for x, y in loader:\n",
    "      with torch.no_grad():\n",
    "        y_scores, _ = model(x)\n",
    "        loss = criterion(y_scores.view(y.size(0) * y.size(1), -1), y.view(y.size(0) * y.size(1)))\n",
    "        total_loss += loss.item()\n",
    "        num += len(y)\n",
    "    return total_loss / num, math.exp(total_loss / num)\n",
    "\n",
    "perf(model, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.3 - Training \n",
    "\n",
    "**Hint:**  Training is the same as for sequence tagging (Section 4 of Exercise Sheet 17)  modulo the added argument returned by the model.\n",
    "* Define the training loop\n",
    "* call you training function to iterate 5 times (epochs) over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating a title (PROVIDED)\n",
    "\n",
    "Write a function which takes the model as input and generates a title.\n",
    "  \n",
    "Reminder: at each time step t, the input to the model is the previous character $c_{t-1}$ and the previous hidden state $h_{t-1}$. So to initialise generation, you need to input  model with a the tensor for that letter and an initial hidden state of size hidden_size. \n",
    "\n",
    "* The inital hidden state should be a tensor of size  (num_layers * num_directions, batch size, hidden_size)\n",
    " with 1 the batch size (only one input in that batch)\n",
    "* The initial character tensor should be of size (1,1) (one row, one char) and contain the integer for the < start > symbol. \n",
    "\n",
    "To generate\n",
    "* use these two tensors as input to the model\n",
    "* Use torch max used to compute predictions (see previous exercise sheet)\n",
    "* break if the selected char (the prediction) is < eos >\n",
    "* else print out the corresponding character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_most_probable(model):\n",
    "    # size for hidden: (batch, num_layers * num_directions, hidden_size)\n",
    "    hidden = torch.zeros(2, 1, hidden_size)\n",
    "    print(hidden.size())\n",
    "    x = torch.zeros(1, 1).long()\n",
    "    x[0, 0] = char2int['<start>']\n",
    "    \n",
    "    for i in range(100):\n",
    "      with torch.no_grad():\n",
    "        y_scores, hidden = model(x, hidden)\n",
    "        y_pred = torch.max(y_scores, 2)[1]\n",
    "        selected = y_pred.data[0, 0]\n",
    "        if selected == char2int['<eos>']:\n",
    "            break\n",
    "        print(int2char[selected.item()], end='')\n",
    "        x[0, 0] = selected\n",
    "    print()\n",
    "\n",
    "generate_most_probable(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
