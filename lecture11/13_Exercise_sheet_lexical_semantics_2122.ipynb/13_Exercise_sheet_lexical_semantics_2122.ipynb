{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise \"Lecture 11: Lexical Semantics\"\n",
    "\n",
    "\n",
    "In this set of exercises, we will convert words to vectors representing their distributional properties. \n",
    "\n",
    "We will start by building a word cooccurrences matrix from the Wikipedia corpus. \n",
    "\n",
    "We will then use Gensim and sklearn predefined methods to build an SVD word context matrix from the Wikipedia corpus used in the preceding two lectures. Finally we'll use cosine to compare the similarity between different pairs of words. \n",
    "\n",
    "The exercises cover the following points:\n",
    "\n",
    "* Converting a corpus to a list of integers where each integer represent a token (this is needed for efficient computation)\n",
    "* Computing a word frequency distribution \n",
    "* Creating word coocurrence matrices\n",
    "* Applying SVD decomposition\n",
    "* Finding neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a corpus from a set of files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** Store all files in 'data/wkp/' into a pandas dataframe with column Text where each row contains the content of one file\n",
    "\n",
    "* use os.scandir to list the files in the directory, read each file into a list of strings (one string per file)   \n",
    "_**Cheatsheet:**_ python_basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/experiments/cours nlp/data science/lecture11/wkp_sorted\")\n",
    "data_folder = os.listdir()\n",
    "wiki_data = []\n",
    "for d in data_folder:\n",
    "    path = os.listdir(os.path.join(os.getcwd(), d))\n",
    "    for text_data in path:\n",
    "        with open(os.path.join(os.getcwd(),d,text_data),\"r\",encoding=\"utf-8\") as f:\n",
    "            data = f.readlines()\n",
    "            wiki_data.append([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Airports of Serbia (Serbian Cyrillic: Аеродро...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[An airport authority is an independent entity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[An airport bus, or airport shuttle bus or air...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Airport check-in is the process whereby passe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Airport security refers to the techniques and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>[Al-Wasat (Arabic: الوسط), also Alwasat, was a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>[The Burj Al Arab (Arabic: برج العرب, Tower of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>[Al-Fazl ( Urdu الفضل) has been the most impor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>[Al HaMishmar (Hebrew: על המשמר, On Guard) was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>[Al Muharrir (Arabic: المُحَرِّر meaning the L...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0    [Airports of Serbia (Serbian Cyrillic: Аеродро...\n",
       "1    [An airport authority is an independent entity...\n",
       "2    [An airport bus, or airport shuttle bus or air...\n",
       "3    [Airport check-in is the process whereby passe...\n",
       "4    [Airport security refers to the techniques and...\n",
       "..                                                 ...\n",
       "155  [Al-Wasat (Arabic: الوسط), also Alwasat, was a...\n",
       "156  [The Burj Al Arab (Arabic: برج العرب, Tower of...\n",
       "157  [Al-Fazl ( Urdu الفضل) has been the most impor...\n",
       "158  [Al HaMishmar (Hebrew: על המשמר, On Guard) was...\n",
       "159  [Al Muharrir (Arabic: المُحَرِّر meaning the L...\n",
       "\n",
       "[160 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki = pd.DataFrame(wiki_data, columns= ['text'])\n",
    "wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/experiments/cours nlp/data science/lecture11/wkp_sorted\"\n",
    "#for l in os.scandir(path):\n",
    "   # print(l.is_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki['text'] = wiki['text'].apply(lambda x : \"\".join(x))\n",
    "text_ = wiki['text'].str.cat(sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** Clean the corpus\n",
    "\n",
    "* Store the content of the 'Text\" column into a string (cf. Pandas CS, \"Extracting all text from a colum\")\n",
    "* Tokenize the string into words (cf. NLTK CS)\n",
    "* Print out the first and the last 10 words of your list of tokens. Do you see tokens that may not be useful for learning word representations ?\n",
    "* Lower case all tokens and remove all tokens that contains characters that are not letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = word_tokenize(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Airports', 'of', 'Serbia', '(', 'Serbian', 'Cyrillic', ':', 'Аеродроми', 'Србије', ')']\n",
      "['was', 'first', 'published', 'in', 'May', '1983', '.', '==', 'References', '==']\n"
     ]
    }
   ],
   "source": [
    "print(word[:10])\n",
    "print(word[-10:])\n",
    "word = [l.lower() for l in word if l.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airports', 'of', 'serbia', 'serbian', 'cyrillic', 'аеродроми', 'србије', 'is', 'a', 'serbian']\n"
     ]
    }
   ],
   "source": [
    "print(word[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a frequency distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** Create a frequency distribution from the list of tokens created in the previous exercise. Print out the 10 most and least frequent tokens.\n",
    "\n",
    "_**Cheatsheet:**_ lexical semantics and stats_and_visu \n",
    "* Create a frequency distribution by iterating over the list of token while incrementing each token frequency accordingly\n",
    "* Sort the tokens by decreasing frequency into a list\n",
    "* Print out the first and last 10 tokens of this list (which tokens are most and least frequent ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "freqdist = Counter(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First ten frequent tokens :     0      1\n",
      "0  the  11652\n",
      "1   of   5443\n",
      "2  and   4677\n",
      "3   in   4564\n",
      "4   to   3773\n",
      "5    a   3479\n",
      "6   is   1633\n",
      "7  was   1596\n",
      "8   as   1495\n",
      "9  for   1284\n",
      " Last ten frequent tokens             0  1\n",
      "0     mustafa  1\n",
      "1    karchawi  1\n",
      "2        abed  1\n",
      "3       jabri  1\n",
      "4  abdelkerim  1\n",
      "5       mouti  1\n",
      "6      jailed  1\n",
      "7        alam  1\n",
      "8     ittihad  1\n",
      "9   ichtiraki  1\n"
     ]
    }
   ],
   "source": [
    "print(f\"First ten frequent tokens :{pd.DataFrame(freqdist.most_common()[:10])}\\n \\\n",
    "Last ten frequent tokens {pd.DataFrame(freqdist.most_common()[-10:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the corpus to a list of integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:** Convert the list of tokens created in Exercise 2 (corpus cleaning) into a list of integers\n",
    "\n",
    "* Create a dictionary mapping each token to a distinct integer (Cf. Lexical Semantics CS)\n",
    "* Use this dictionary to convert each token from your cleaned corpus (Exercise 2) into an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2int = defaultdict(lambda: len(token2int)) #can we use enumerate to do that? \n",
    "token2int['<eos>'] = 0\n",
    "for text in word:\n",
    "    [token2int[token] for token in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token2int.keys()# a function to understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary of co-occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:** In the previous exercise, you created a list of integers where each integer is the identifier for the corresponding token in your cleaned up corpus. Iterate over that list and for each \"integer token\" *i*:\n",
    "\n",
    "* get the neihbours of *i* within a window of size 5 (only looking at the right side of *i*)\n",
    "* store these neihbours in a dictionary of coocurrences of the form {(i,j):f,} where *(i,j)* are neighbours and *f* is the frequency of the co-occurence \n",
    "* Sort co-occurences using integer order i.e., if the neighbour *n* is represented by an identifier smaller than *i*, store the co-occurence as *(n,i)*, otherwise as *(i,n)* .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrences = defaultdict(int)\n",
    "\n",
    "for i, token in enumerate(word[:-5]):\n",
    "    for j in range(1, 6):\n",
    "        neighbour_token = word[i + j]\n",
    "        cooccurrences[(min(token2int[token], token2int[neighbour_token]),\n",
    "                       max(token2int[token], token2int[neighbour_token]))] += 1\n",
    "cooccurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the SVD decomposition of the Co-occurence Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:** Compute the  SVD decomposition of the word co-occurence matrix you just created\n",
    "\n",
    "* Create a matrix A of size (vocab_length, vocab_length)\n",
    "* Fill each cell *(i,j)* in this matrix with the frequency the co-occurrence between *i* and *j*\n",
    "* Use numpy [svd](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html) method from the linalg module\n",
    "* full_matrices = False ensures that reduced SVD is applied (rather than full SVD)\n",
    "\n",
    "A = U * s * V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "       [  0.,   9.,  37., ...,   0.,   0.,   0.],\n",
       "       [  0.,   0., 628., ...,   0.,   0.,   0.],\n",
       "       ...,\n",
       "       [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0., ...,   0.,   0.,   1.],\n",
       "       [  0.,   0.,   0., ...,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix = np.zeros((len(token2int), len(token2int)))\n",
    "for (i, j), value in cooccurrences.items():\n",
    "    matrix[i, j] = value\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, V = np.linalg.svd(matrix, full_matrices=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7 (PROVIDED):** Define a function which returns the similarity between two words and apply to measure the similarity of airport and news, airport and international, john and peter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def similarity(embedding, word1, word2):\n",
    "  if word1 in vocab and word2 in vocab:\n",
    "    v1 = embedding[token2int[word1]].reshape(1, -1)  \n",
    "    v2 = embedding[token2int[word2]].reshape(1, -1)\n",
    "    return cosine_similarity(v1, v2)[0][0]\n",
    "\n",
    "print('cosine(airport, news) =', similarity(U, 'airport', 'news'))\n",
    "print('cosine(airport, international) =', similarity(U, 'airport', 'international'))\n",
    "print('cosine(john, peter) =', similarity(U, 'john', 'peter'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8 (PROVIDED):** Define a function which outputs the neighbours of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_vocab = {j: i for i, j in token2int.items()}\n",
    "\n",
    "def most_similar(embedding, word, n=10):\n",
    "  if word in vocab:\n",
    "    v = embedding[token2int[word]].reshape(1, -1)\n",
    "    scores = cosine_similarity(v, embedding).reshape(-1)\n",
    "    result = []\n",
    "\n",
    "    # argsort gives n-best scores\n",
    "    for i in reversed(scores.argsort()[-n:]):\n",
    "      result.append((reverse_vocab[i], scores[i]))\n",
    "    return result\n",
    "\n",
    "print(most_similar(U, 'airport'))\n",
    "print(most_similar(U, 'news'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating a word cooccurence matrix using vectorizers\n",
    "\n",
    "In the preceding section, we created the word co-occurence matrix programatically (we wrote the algorithm for deriving the matrix from the corpus). There is in fact a much quicker way to do this which can be summarised as follows:\n",
    "\n",
    "* Use sklearn vectorizer methods (CountVectorizer, TfidfVectorizer) to convert the corpus (a list of documents) to a _**document/token matrix**_\n",
    "* Use algebra to create the token/token matrix.  To create a _**token co-occurence matrix**_ , we simply multiply the transpose of the documents/tokens matrix by the documents/token matrix\n",
    "    * shape of X: (#doc, #tokens)   \n",
    "    * shape of X transpose: (#tokens, #doc)   \n",
    "    * shape of X transpose * X : (#tokens, #doc) * (#doc, #tokens) = (#tokens, #tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9:** Convert the Wikipedia files into a list of strings and preprocess each string\n",
    "\n",
    "* Convert the Text column of the dataframe created in Exercise 1 into a list of strings\n",
    "* Define a preprocessing function which takes a list of strings as input, tokenizes each string, lowercases the tokens, only keep tokens made of letters (use isalpha() method), convert the list of cleaned tokens back into a string (use \"join\") and stores the result into a lists of preprocessed strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_list = []\n",
    "for text in wiki['text']:\n",
    "    str_list.append(text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "    text_prep = []\n",
    "    for text in s:\n",
    "        word = word_tokenize(text)\n",
    "        word = [l.lower() for l in word if l.isalpha()]\n",
    "        text_prep.append(\" \".join(word))\n",
    "    return text_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = preprocess(str_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 10:** Creating a document / token matrix\n",
    "\n",
    "* Use sklearn [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) method which transforms a list of documents (strings) into a a document/token matrix where each cell indicates the frequency of a token in a document\n",
    "* Use the stop_words option to remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_model = CountVectorizer(ngram_range=(1,1), stop_words = 'english')\n",
    "X = count_model.fit_transform(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 11:** Print out the vocabulary i.e., the tokens contained in the document/token matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_model.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 12:** Create the co-occurence matrix.\n",
    "\n",
    "To create a token co-occurence matrix, we simply multiply the transpose of the documents/tokens matrix by the documents/token matrix\n",
    "\n",
    "* shape of X: (#doc, #tokens)\n",
    "* shape of X transpose: (#tokens, #doc)\n",
    "* shape of X transpose * X : (#tokens, #doc) * (#doc, #tokens) = (#tokens, #tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_term = X@X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 13:** Use the function given in Exercise 7 to measure the similarity of airport and news, airport and international, john and peter\n",
    "\n",
    "* You'll need to modify that part of the function which retrieves the identifier of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def similarity(embedding, word1, word2):\n",
    "    v1 = embedding[token2int[word1]].reshape(1, -1)  \n",
    "    v2 = embedding[token2int[word2]].reshape(1, -1)\n",
    "    return cosine_similarity(v1, v2)[0][0]\n",
    "\n",
    "print('cosine(airport, news) =', similarity(U, 'airport', 'news'))\n",
    "print('cosine(airport, international) =', similarity(U, 'airport', 'international'))\n",
    "print('cosine(john, peter) =', similarity(U, 'john', 'peter'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
