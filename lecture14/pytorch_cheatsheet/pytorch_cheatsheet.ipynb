{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Cheatsheet\n",
    "\n",
    "[Pytorch Documentation](https://pytorch.org/docs/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a Torch tensor\n",
    "t = torch.Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Construct a matrix filled zeros and of dtype long:\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2548, -0.3908,  0.8081],\n",
       "        [-0.4188,  0.7338,  0.3413],\n",
       "        [-0.5131,  0.6303,  1.9887]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tensor from normal distribution randoms\n",
    "t = torch.randn(3, 3)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspecting tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.zeros((1, 1)).long()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape: torch.Size([3, 4, 2])\n",
      "Number of dimensions: 3\n",
      "Tensor type: torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# Some tensor info\n",
    "t = torch.randn(3, 4, 2)\n",
    "print('Tensor shape:', t.shape)   # t.size() gives the same\n",
    "print('Number of dimensions:', t.dim())\n",
    "print('Tensor type:', t.type())   # there are other types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "torch.strided\n"
     ]
    }
   ],
   "source": [
    "print(t.dtype) # Type of the data contained within the tensor.\n",
    "print(t.device) # where tensor computations will be performed (CPU or GPU)\n",
    "print(t.layout) # how the tensor is stored in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of elements contained within the tensor\n",
    "t.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reshaping Tensors**\n",
    "\n",
    "Reshaping changes the tensor's shape but not the underlying data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2331,  1.4226, -0.3856,  0.5372],\n",
       "        [-1.0866,  0.3230,  0.5670, -0.3185],\n",
       "        [ 2.0182, -0.3583,  1.5225, -0.2749]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(3, 4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2331,  1.4226, -0.3856,  0.5372, -1.0866,  0.3230],\n",
       "        [ 0.5670, -0.3185,  2.0182, -0.3583,  1.5225, -0.2749]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape([2,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2331,  1.4226],\n",
       "        [-0.3856,  0.5372],\n",
       "        [-1.0866,  0.3230],\n",
       "        [ 0.5670, -0.3185],\n",
       "        [ 2.0182, -0.3583],\n",
       "        [ 1.5225, -0.2749]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape([6,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8118, -1.0233, -0.3762],\n",
       "        [ 0.9262,  1.9108, -0.5269],\n",
       "        [ 0.9078, -0.5441,  0.2761],\n",
       "        [-0.2772,  2.1320, -0.2285]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape([4,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8118, -1.0233, -0.3762,  0.9262],\n",
       "        [ 1.9108, -0.5269,  0.9078, -0.5441],\n",
       "        [ 0.2761, -0.2772,  2.1320, -0.2285]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.8118, -1.0233, -0.3762],\n",
       "         [ 0.9262,  1.9108, -0.5269]],\n",
       "\n",
       "        [[ 0.9078, -0.5441,  0.2761],\n",
       "         [-0.2772,  2.1320, -0.2285]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " t.reshape(2,2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resizing using torch.view**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View again...\n",
    "x = t.view(6,2)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.2331,  1.4226],\n",
      "         [-0.3856,  0.5372]],\n",
      "\n",
      "        [[-1.0866,  0.3230],\n",
      "         [ 0.5670, -0.3185]],\n",
      "\n",
      "        [[ 2.0182, -0.3583],\n",
      "         [ 1.5225, -0.2749]]])\n"
     ]
    }
   ],
   "source": [
    "y = x.view(3, 2, 2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2331,  1.4226],\n",
      "        [-0.3856,  0.5372],\n",
      "        [-1.0866,  0.3230],\n",
      "        [ 0.5670, -0.3185],\n",
      "        [ 2.0182, -0.3583],\n",
      "        [ 1.5225, -0.2749]])\n"
     ]
    }
   ],
   "source": [
    "print(y.view(6, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Squeezing and unsqueezing**\n",
    "\n",
    "* Squeezing a tensor removes the dimensions or axes that have a length of one.\n",
    "\n",
    "* Unsqueezing a tensor adds a dimension with a length of one.\n",
    "\n",
    "* These functions allow us to expand or shrink the rank (number of dimensions) of our tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n",
      "tensor([-1.2331,  1.4226, -0.3856,  0.5372, -1.0866,  0.3230,  0.5670, -0.3185,\n",
      "         2.0182, -0.3583,  1.5225, -0.2749])\n",
      "torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "print(t.reshape([1,12]).shape)\n",
    "print(t.reshape([1,12]).squeeze())\n",
    "print(t.reshape([1,12]).squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randn(3,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 3])\n",
      "torch.Size([3, 2, 1, 3])\n",
      "torch.Size([3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())\n",
    "y3 = y.unsqueeze(2)\n",
    "print(y3.size())\n",
    "print(y3.squeeze().size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7, 17, 13,  2,  1, 17],\n",
       "        [ 6, 13, 15, 17,  2,  5],\n",
       "        [ 6, 12, 11,  8, 15,  0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transpose**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1132,  0.3919],\n",
       "        [ 2.1337, -0.3215],\n",
       "        [ 0.1802, -0.0220]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Broadcasting**\n",
    "\n",
    "[See](http://pytorch.org/docs/0.3.1/notes/broadcasting.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7466],\n",
      "        [0.2970],\n",
      "        [0.2107]])\n",
      "tensor([[0.1799, 0.3676],\n",
      "        [0.2930, 0.1088],\n",
      "        [0.1289, 0.3924]])\n",
      "tensor([[0.9264, 1.1142],\n",
      "        [0.5900, 0.4059],\n",
      "        [0.3396, 0.6031]])\n"
     ]
    }
   ],
   "source": [
    "# broadcasting\n",
    "x = torch.rand(3, 1)\n",
    "y = torch.rand(3, 2)\n",
    "print(x)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Tensor Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2104, 0.2674, 0.1711],\n",
      "        [0.7613, 0.5942, 0.4864],\n",
      "        [0.8272, 0.6107, 0.5162]])\n",
      "m tensor([[[0.5248, 0.1521, 0.0502],\n",
      "         [0.8608, 0.2549, 0.0874],\n",
      "         [0.4297, 0.1748, 0.0879]],\n",
      "\n",
      "        [[0.5194, 0.5558, 0.9434],\n",
      "         [0.4951, 0.4243, 0.8227],\n",
      "         [0.3243, 0.3828, 0.6149]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[100.],\n",
       "        [250.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute matrix product\n",
    "x = torch.rand(3, 2)\n",
    "y = torch.rand(2, 3)\n",
    "print(x.matmul(y))\n",
    "\n",
    "x = torch.rand(2, 3, 2)\n",
    "y = torch.rand(2, 2, 3)\n",
    "print(\"m\",x.bmm(y))\n",
    "\n",
    "t = (torch.Tensor([[2, 4], [5, 10]]).mm(torch.Tensor([[10], [20]])))\n",
    "t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatenating tensors**\n",
    "\n",
    "We combine tensors using the cat() function, and the resulting tensor will have a shape that depends on the shape of the two input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[5, 6],\n",
      "        [7, 8]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1,2],\n",
    "    [3,4]\n",
    "])\n",
    "t2 = torch.tensor([[5,6],[7,8]])\n",
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6],\n",
       "        [7, 8]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine row wize\n",
    "torch.cat((t1, t2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 5, 6],\n",
       "        [3, 4, 7, 8]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine columns wize\n",
    "torch.cat((t1, t2), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stacking tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1]\n",
    "])\n",
    "\n",
    "t2 = torch.tensor([\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2]\n",
    "])\n",
    "\n",
    "t3 = torch.tensor([\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 4])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.stack((t1, t2, t3))\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting Tensor Values** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 6., 9.])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([[9.]])\n"
     ]
    }
   ],
   "source": [
    "# Slicing\n",
    "t = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Every row, only the last column\n",
    "print(t[:, -1])\n",
    "\n",
    "# First 2 rows, all columns\n",
    "print(t[:2, :])\n",
    "\n",
    "# Lower right most corner\n",
    "print(t[-1:, -1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you have a one element tensor, use .item() to get the value as a Python number\n",
    "\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Max value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n",
      "tensor([[4],\n",
      "        [9]])\n",
      "tensor([[4],\n",
      "        [4]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0,10).resize_((2,5))\n",
    "\n",
    "# Return max values and their position \n",
    "topk, indices = torch.topk(x, 1)\n",
    "\n",
    "print(x)\n",
    "print(topk)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch Tensor To and From Numpy ndarray**\n",
    "\n",
    "You can easily create a tensors from an ndarray and vice versa. These operations are fast, since the data of both structures will share the same memory space, and so no copying is involved. This is obviously an efficient approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Numpy ndarray <--> PyTorch tensor\n",
    "import numpy as np\n",
    "\n",
    "# ndarray to tensor\n",
    "a = np.random.randn(3, 5)\n",
    "t = torch.from_numpy(a)\n",
    "print(a)\n",
    "print(t)\n",
    "print(type(a))\n",
    "print(type(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### GPUs\n",
    "\n",
    "PyTorch tensors have inherent GPU support. Specifying to use the GPU memory and CUDA cores for storing and performing tensor calculations is easy; the cuda package can help determine whether GPUs are available, and the package's cuda() method assigns a tensor to the GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether GPU is available\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# Move to GPU\n",
    "t.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling and batching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# the TensorDataset is a ready to use class to represent your data as list of tensors. \n",
    "# Note that input_features and labels must match on the length of the first dimension\n",
    "train_set = TensorDataset(X_train, Y_train)\n",
    "valid_set = TensorDataset(X_valid, Y_valid)\n",
    "\n",
    "# DataLoader shuffles and batches the data and load its in parallel using multiprocessing workers\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Within the __init__ we define the layers of the module. \n",
    "\n",
    "Our three layers are an embedding layer, our RNN, and a linear layer. All layers have their parameters initialized to random values, unless explicitly specified.\n",
    "\n",
    "The embedding layer is used to transform our sparse one-hot vector (sparse as most of the elements are 0) into a dense embedding vector (dense as the dimensionality is a lot smaller and all the elements are real numbers). This embedding layer is simply a single fully connected layer. As well as reducing the dimensionality of the input to the RNN, there is the theory that words which have similar impact on the sentiment of the review are mapped close together in this dense vector spac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import sigmoid\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Training data and ground truth\n",
    "x_data = tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y_data = tensor([[0.], [0.], [1.], [1.]])\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        create an instance of the base nn.Module class\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data.\n",
    "        \"\"\"\n",
    "        y_pred = sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(f'Epoch {epoch + 1}/1000 | Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# After training\n",
    "print(f'\\nLet\\'s predict the hours need to score above 50%\\n{\"=\" * 50}')\n",
    "hour_var = model(tensor([[1.0]]))\n",
    "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')\n",
    "hour_var = model(tensor([[7.0]]))\n",
    "print(f'Prediction after 7 hours of training: {hour_var.item():.4f} | Above 50%: { hour_var.item() > 0.5}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import sigmoid\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# Training data and ground truth\n",
    "x_data = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y_data = torch.tensor([[0.], [0.], [1.], [1.]])\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # One in and one out\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data.\n",
    "        \"\"\"\n",
    "        y_pred = sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "# our model\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "  \n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Here we define the network layers\n",
    "        \n",
    "        # An embedding layer projecting vectors of size vocab_size into embeddings of size embed_size\n",
    "        # Assigns to each word in the vocabulary an embedding of size embed_size\n",
    "        self.embed = nn.Embedding(len(vocab), embed_size)\n",
    "        \n",
    "        # A recurrent (GRU) layer to process each input token (represented by its embedding)\n",
    "        # The GRU network takes as input the embedding (of size embed_size) of the current word \n",
    "        # and the previous hidden state (of size hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers=1, bidirectional=False, batch_first=True)\n",
    "        \n",
    "        # Drop out layer for regularisation\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Fully connected layer mapping \n",
    "        # the last layer maps a hidden state to a vector of size the number of classes\n",
    "        self.decision = nn.Linear(hidden_size, len(set(labels)))\n",
    "         \n",
    "    def forward(self, x):\n",
    "        # Here we say how the layers are connected \n",
    "       \n",
    "        #  for each token in the input, retrieve the corresponding embeddings \n",
    "        # x = [batch size, max sent length]\n",
    "        embed = self.embed(x)\n",
    "        \n",
    "       \n",
    "        # Run the RNN on the input embeddings\n",
    "        # embed = [batch size, sent len, emb dim]\n",
    "        # output is the sequence of hidden states produced by the RNN\n",
    "        # hidden is the last hidden state produced\n",
    "        output, hidden = self.rnn(embed)\n",
    "        \n",
    "        # output = [sent len, batch size, hidden size]\n",
    "        # hidden = [1, batch size, hidden size]\n",
    "        # hidden = (num_layers * num_directions, batch, hidden_size)\n",
    "        \n",
    "        # Apply dropout (for regularisation)\n",
    "        drop = self.dropout(hidden)\n",
    "        \n",
    "        # Apply the fully connected layer to the output of the dropout\n",
    "        # drop = (num_layers * num_directions, batch_size, hidden_size)\n",
    "        # hidden size: (num_layers * num_directions, batch, hidden_size)\n",
    "        # Expected input size: (batch_size, _size)\n",
    "        return self.decision(drop.transpose(0, 1).contiguous().view(x.size(0), -1))\n",
    "        #return self.decision(drop)\n",
    "    \n",
    "rnn_model = RNN()\n",
    "rnn_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer\n",
    "\n",
    "* The nn.Embedding module holds a Tensor of dimension (vocab_size, embedding_size), i.e. of the size of the vocabulary x the dimension of each vector embedding, and a method for retrieving the embedding of a word. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# Create an embedding layer\n",
    "# Parameters: (vocab_size, embedding_size)\n",
    "embedding = nn.Embedding(1000,128)\n",
    "# Print out the embedding of the token represented by index 3\n",
    "embedding(torch.LongTensor([3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer\n",
    "\n",
    "Parameters:\n",
    "\n",
    "* in_features – size of each input sample\n",
    "* out_features – size of each output sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#x contains three inputs (i.e. the batch size is 3),\n",
    "x = torch.tensor([[1.0, -1.0],\n",
    "                  [0.0,  1.0],\n",
    "                  [0.0,  0.0]])\n",
    "\n",
    "in_features = x.shape[1]  # = 2\n",
    "out_features = 1\n",
    "\n",
    "# input x of shape (batch_size, in_features)\n",
    "# output of shape (batchsize, out_features)\n",
    "m = nn.Linear(in_features, out_features)\n",
    "\n",
    "# output\n",
    "y = m(x)\n",
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
