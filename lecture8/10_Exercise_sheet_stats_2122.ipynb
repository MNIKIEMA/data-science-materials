{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises \"Lecture 8: Exploratory Data Analysis and Visualisation\"\n",
    "\n",
    "In this session, we will compute statistics and visualizations on Wikipedia articles from 16 categories: \n",
    "\n",
    "Airports, Artists, Astronauts, Astronomical_objects, Building,City,Comics_characters, Companies,Foods, Monuments_and_memorials,Politicians,Sports_teams,Sportspeople, Transport, \n",
    "Universities_and_colleges, Written_communication.\n",
    "\n",
    "Data: wkp directory         \n",
    "Python libraries\n",
    "- os, to list files in a directory\n",
    "- re, regexp library\n",
    "- pandas\n",
    "- spacy (or Stanza)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading text files into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1** \n",
    "\n",
    "* Use a regexp and the list of categories (given above and pasted below) to split each file name in the wkp/ directory into id and category as illustrated below (cf. regexp CS)\n",
    "\n",
    "```\n",
    "File Name: Monteverde_Angel_Monuments_and_memorials\n",
    "id: Monteverde_Angel\n",
    "category: Monuments_and_memorials\n",
    "```\n",
    "\n",
    "* Extract the content of the file (use read(), cf. python_basics CS))\n",
    "\n",
    "* Create a list of lists of the form (id, category, file_content) e.g., \n",
    "\n",
    "```\n",
    "[['Monteverde_Angel', 'Monuments_and_memorials', 'The Monteverde Angel or Angel of the Resurrect ....], ...]\n",
    "```\n",
    "\n",
    "* Create a dataframe from this list of lists with headers 'Id', 'Category' and'Text' (cf. pandas CS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Categories = ['Airports', 'Artists', 'Astronauts', 'Astronomical_objects', 'Building','City','Comics_characters', 'Companies',\\\n",
    "              'Foods', 'Monuments_and_memorials','Politicians','Sports_teams','Sportspeople', 'Transport', \\\n",
    "              'Universities_and_colleges', 'Written_communication']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\experiments\\\\cours nlp\\\\data science\\\\lecture8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('wkp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [l.replace('.txt','') for l in os.listdir()[1:]]\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aisam-ul-Haq_Qureshi_career_statistics'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = file_list[1].endswith('Airports')\n",
    "file_list[9].rstrip(\"Sportspeople\")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpk = []\n",
    "for l in file_list:\n",
    "    with open(str(l + \".txt\"), 'r',encoding=\"utf8\") as f:\n",
    "        content = str(f.readlines()).strip('[]')\n",
    "    for c in Categories:\n",
    "        if l.endswith(c):\n",
    "            categorie = c\n",
    "            id_ = l.rstrip(c)[:-1]\n",
    "            wpk.append([id_, categorie, content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = pd.DataFrame(wpk, columns = ['ids','category','content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Airports_of_Serbia</td>\n",
       "      <td>Airports</td>\n",
       "      <td>'Airports of Serbia (Serbian Cyrillic: Аеродро...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Airport_authority</td>\n",
       "      <td>Airports</td>\n",
       "      <td>'An airport authority is an independent entity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Airport_bus</td>\n",
       "      <td>Airports</td>\n",
       "      <td>'An airport bus, or airport shuttle bus or air...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Airport_check-in</td>\n",
       "      <td>Airports</td>\n",
       "      <td>\"Airport check-in is the process whereby passe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Airport_security</td>\n",
       "      <td>Airports</td>\n",
       "      <td>'Airport security refers to the techniques and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ids  category  \\\n",
       "0  Airports_of_Serbia  Airports   \n",
       "1   Airport_authority  Airports   \n",
       "2         Airport_bus  Airports   \n",
       "3    Airport_check-in  Airports   \n",
       "4    Airport_security  Airports   \n",
       "\n",
       "                                             content  \n",
       "0  'Airports of Serbia (Serbian Cyrillic: Аеродро...  \n",
       "1  'An airport authority is an independent entity...  \n",
       "2  'An airport bus, or airport shuttle bus or air...  \n",
       "3  \"Airport check-in is the process whereby passe...  \n",
       "4  'Airport security refers to the techniques and...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the list of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2** \n",
    "    \n",
    "- store the content of the Category column into a string (cf. Pandas CS)\n",
    "- extract the uniq tokens from that string (cf. python basic CS)   \n",
    "You should find the following 16 categories\n",
    "\n",
    "```\n",
    "['Comics_characters', 'Astronauts', 'Transport', 'Artists', 'Written_communication', 'Sports_teams', 'Foods', 'Airports', 'Monuments_and_memorials', 'Politicians', 'Sportspeople', 'Building', 'Universities_and_colleges', 'Astronomical_objects', 'Companies', 'City']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Airports',\n",
       " 'Transport',\n",
       " 'Sportspeople',\n",
       " 'Monuments_and_memorials',\n",
       " 'Foods',\n",
       " 'Building',\n",
       " 'Artists',\n",
       " 'Universities_and_colleges',\n",
       " 'Politicians',\n",
       " 'Companies',\n",
       " 'Written_communication',\n",
       " 'Astronomical_objects',\n",
       " 'Comics_characters',\n",
       " 'Astronauts',\n",
       " 'City',\n",
       " 'Sports_teams']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(wiki_data['category'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the list of headers from the 'Text' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3** \n",
    "\n",
    "In the Wikipedia articles, headers are surrounded by \"==\" \n",
    "\n",
    "_*E.g., ==  Background == *_\n",
    "\n",
    "- Define a function which extracts headers from a text (Use a regular expression)\n",
    "- Apply this function to the Text column in your panda frame (use pandas 'apply' method)\n",
    "- Store the result (the list of headers associated with each text in the frame) into a new pandas serie called 'Headers'\n",
    "- Concatenate this serie to your pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_extract(text):\n",
    "    return re.findall(r\"={2}(.+?)={2}\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"={2}(.+?)={2}\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' On airport transfer ',\n",
       " '= Airside transfer ',\n",
       " '= Terminal transfer ',\n",
       " '= Car park transfer ',\n",
       " ' Off airport transfer ',\n",
       " ' Public bus services ',\n",
       " '= Intermodal shuttle buses ',\n",
       " '= Premium airlink services ',\n",
       " '= Demand responsive shuttle buses ',\n",
       " '= Express services ',\n",
       " '= Terminal transfer ',\n",
       " '= Railair ',\n",
       " ' Other non-airport bus services ',\n",
       " '= Standard bus services ',\n",
       " '= Private hire ',\n",
       " ' See also ',\n",
       " ' References ',\n",
       " ' External links ']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_extract(wiki_data.content[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extracting the Vocabulary of each Category\n",
    "\n",
    "For each category, we extract the corresponding vocabulary i.e., the list of tokens occurring in the corresponding texts (removing the duplicates)\n",
    "\n",
    "\n",
    "Optional: for each category\n",
    "- extract the list of headers\n",
    "- extract the noun and verbs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**\n",
    "\n",
    "- Define a function 'get_tokens' which, given a category, return its vocabulary (the list of tokens occurring in the texts of that category and after removing the duplicates). One way to do this is to:\n",
    "   - extract the category subframe i.e., all rows whose category column matches the input category\n",
    "   - create a string out of the text column of that subframe (use str.cat(sep=\" \"), cf. Pandas CS)\n",
    "   - run SpaCy or Stanza model on this string and extract the tokens from the resulting document (cf. pstanza or spacy CS)\n",
    "   - use python set method to remove duplicate tokens\n",
    "   - use python list method to convert the resulting set back into a list\n",
    "- Create a new dataframe with headers 'CATEGORY' and 'VOCABULARY' in which you store for each category the corresponding vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    s = wiki_data.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the differences in Vocabulary Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**\n",
    "\n",
    "- Use pandas 'apply' method to compute for each vocabulary its size (the number of tokens)\n",
    "- Add a 'VOCAB SIZE' column to your the dataframe created in the previous exercise in which you input the size of the vocabulary for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply with lambda method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6**\n",
    "\n",
    "Create a Barplot showing the Vocabulary Size of each Category (use e.g., pandas barh)\n",
    "\n",
    "- the y axis should show the categories\n",
    "- the x axis should show the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
