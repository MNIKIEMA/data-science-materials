{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sB9EwaZbbKMN"
   },
   "source": [
    "# Exercise Lecture \"15: Neural Sequence Tagging\"\n",
    "\n",
    "In this assignement, we learn a model which can detect noun phrases referring to visual entities given the Flicker30k entities corpus as training data.\n",
    "\n",
    "In this corpus, each word is labelled with either (B) if that word starts an NP, (I) if it occurs within an NP and (O) otherwise. There is one word and one label per line. Sentences are separated by blank lines. \n",
    "\n",
    "Data file:  f30kE-captions-bio.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\experiments\\cours nlp\\data science\\lecture15\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ecqfYZ4kbKMh"
   },
   "source": [
    "#### Exercise 1 - Creating a list of lists of tokens (one list per sentence) and the corresponding lists of labels\n",
    "\n",
    "* From the input file, create two lists called \"texts\" and \"labels\"\n",
    "* \"text\" is a list of lists, each list containing the tokens of a sentence\n",
    "* \"labels\" contains the list of lists of labels for each sentence in \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "with open('f30kE-captions-bio/f30kE-captions-bio.txt') as f:\n",
    "    # Les mots et labels de la phrase en cours\n",
    "    cur_text = []\n",
    "    cur_labels = []\n",
    "    for line in f: # On traite les lignes une par une\n",
    "        line = line.strip()\n",
    "        if line: # Si la ligne n'est pas vide, on reste sur la même phrase\n",
    "            text, label = line.split()\n",
    "            cur_text.append(text) # Et on lui ajoute le mot et label courants\n",
    "            cur_labels.append(label)\n",
    "        if not line.strip(): # Sinon, on est à la fin d'une phrase\n",
    "            texts.append(cur_text) # On ajoute les mots et labels de la phrase à nos listees globales\n",
    "            labels.append(cur_labels)\n",
    "            cur_text = [] # Et on vide les listes de la phrase courante (on commence une nouvelle phrase)\n",
    "            cur_labels = []\n",
    "print(labels)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NuckS_TsbKMy"
   },
   "source": [
    "#### Exercise 2 -  Mapping labels to integers and sequence of labels to sequence of integers\n",
    "\n",
    "* Create a dictionary label2int which maps each label to a distinct integer\n",
    "* Apply this dictionary to the list of labels extracted in the previous exercise \n",
    "\n",
    "**Hint:** We did this in the preceding lab session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = set() # Un set est comme une liste mais ne peut pas contenir de répétitions et n'a pas d'ordre fixe\n",
    "for sent in labels: # Pour chaque phrase\n",
    "    for label in sent:\n",
    "        all_labels.add(label) # On ajoute tous les labels de notre phrase au set\n",
    "        \n",
    "print(all_labels) # Contient tous les labels uniques du texte (B, I et O)\n",
    "\n",
    "label2int = dict()\n",
    "for i, label in enumerate(all_labels):\n",
    "    label2int[label] = i\n",
    "\n",
    "print(label2int)\n",
    "\n",
    "# J'utilise une compréhension de liste pour simplifier l'écriture. int_labels contient,\n",
    "# pour chaque phrase (for sent in labels), une liste contenant, pour chaque label de la phrase (for label in sent),\n",
    "# le label transformé en entier (label2int[label])\n",
    "# Sinon il aurait fallu faire une boucle similaire à celle de l'exercice précédent\n",
    "int_labels = [[label2int[label] for label in sent] for sent in labels]\n",
    "print(int_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VmX8f6yUbKM5"
   },
   "source": [
    "#### Exercise 3 - Convert the tokens to integers\n",
    "\n",
    "* Similarly define a token2int dictionary mapping each token in your corpus to an integer and use this dictionary to convert the texts in the list \"texts\" (cf. Exercise 1.1) into lists of integers, each integer representing a token\n",
    "\n",
    "* **IMPORTANT** make sure to lowercase the tokens as the pre-trained embeddings we'll be using only include lowercased tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presque exactement pareil que l'exercice précédent\n",
    "all_tokens = set()\n",
    "for sent in texts:\n",
    "    for token in sent:\n",
    "        all_tokens.add(token.lower()) # On passe bien en minuscule\n",
    "        \n",
    "print(all_tokens)\n",
    "\n",
    "token2int = dict()\n",
    "for i, token in enumerate(all_tokens):\n",
    "    token2int[token] = i\n",
    "\n",
    "print(token2int)\n",
    "\n",
    "int_tokens = [[token2int[token.lower()] for token in sent] for sent in texts] # On passe aussi en minuscule ici\n",
    "print(int_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "knZtpGzZbKND"
   },
   "source": [
    "#### Exercise 4 - Create the reverse dictionaries (int2label, int2token) to map integer labels and integer tokens back to labels and tokens \n",
    "\n",
    "This is useful to be able to inspect results later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version \"classique\"\n",
    "int2label = dict()\n",
    "for key, value in label2int.items():\n",
    "    int2label[value] = key # On inverse le dictionnaire, c-a-d les clés et les valeurs\n",
    "    \n",
    "int2token = dict() # Pareil pour les tokens\n",
    "for key, value in token2int.items():\n",
    "    int2token[value] = key\n",
    "\n",
    "    \n",
    "# Version qui utilise une compréhension de dictionnaire pour simplifier les choses\n",
    "int2label = {value: key for key, value in label2int.items()}\n",
    "int2token = {value: key for key, value in token2int.items()}\n",
    "\n",
    "\n",
    "print(int2label)\n",
    "print(int2token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dX_xNnPbKNJ"
   },
   "source": [
    "##### Pytorch import and key constants (PROVIDED)\n",
    "\n",
    "- `max_len` is the maximum sentence length\n",
    "- `batch_size` is the batch size\n",
    "- `embed_size` is the size of the pre-trained embeddings (word vectors). We use fasttext pre-trained embeddings of size 300.\n",
    "- `hidden_size` is the size of the RNN hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CgHjYS72bKNP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "max_len = 16\n",
    "batch_size = 64\n",
    "embed_size = 300\n",
    "hidden_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1  - Creating tensors\n",
    "\n",
    "To work with pytorch, all data must be converted to tensors. \n",
    "\n",
    "* X and Y are tensors of integers of size (number of sentences, max sentence length) initialised with 0 (this is the value of the padding symbol)\n",
    "* For each x and y, compute the length and cut down any instance that is over the max sentence length to that length\n",
    "* We populate the zeros tensors with the input data from exercise 1.1\n",
    "\n",
    "**Hint:** You did this in the previous lab session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5500, 16])\n",
      "torch.Size([5500, 16])\n"
     ]
    }
   ],
   "source": [
    "# On crée un tableau de dimension len(text) × max_len, et on le remplit de 0\n",
    "X = torch.zeros(len(texts), max_len, dtype=torch.long)\n",
    "\n",
    "# On remplit le tensor ligne par ligne avec nos textes convertis en nombres entiers\n",
    "for i, int_text in enumerate(int_tokens): # Pour chaque texte\n",
    "    if len(int_text) < max_len: # Si le texte est trop court\n",
    "        int_text = int_text + [len(token2int)] * (max_len - len(int_text)) # Alors on lui rajoute des tokens vides jusqu'à\n",
    "        # atteindre la bonne longueur\n",
    "    X[i] = torch.LongTensor(int_text[:max_len]) # On remplit la rangée correspondante (et on coupe à max_len tokens)\n",
    "\n",
    "# Même chose pour Y\n",
    "Y = torch.zeros(len(texts), max_len, dtype=torch.long)\n",
    "for i, int_label in enumerate(int_labels):\n",
    "    if len(int_label) < max_len:\n",
    "        int_label = int_label + [len(label2int)] * (max_len - len(int_label))\n",
    "    Y[i] = torch.LongTensor(int_label[:max_len])\n",
    "\n",
    "print(X.size())\n",
    "print(Y.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise  6 - Create train and validation data\n",
    "\n",
    "* Split X into two parts, one called X_train which consists of the first 5000 items and the other called X_valid which includes the rest of the data\n",
    "* Do the same for Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:5000]\n",
    "X_valid = X[5000:]\n",
    "\n",
    "Y_train = Y[:5000]\n",
    "Y_valid = Y[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V1duVtHubKNk"
   },
   "source": [
    "#### Exercise  7 - Use torch DataLoader to split training and validation data into batches\n",
    "\n",
    "**Hint:** This was provided in the previous lab sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copié du TD 16\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_set = TensorDataset(X_train, Y_train)\n",
    "valid_set = TensorDataset(X_valid, Y_valid)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_9dJoanbKNp"
   },
   "source": [
    "## Using pre-trained Fasttext embeddings\n",
    "\n",
    "Instead of learning word embeddings using the network, we use pre-trained Fasttext embeddings. These are available [here](https://fasttext.cc/docs/en/english-vectors.html).\n",
    "\n",
    "However these cover several millions words and the files are large. Instead we'll use a smaller version which is restricted to the corpus vocabulary and is available on arche (wiki.en.filtered.vec). Each line in that file contains a token followed by the Fasttext embedding of that token(300 dimensions). \n",
    "E.g., auditorium -0.054196 -0.37375 ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_9dJoanbKNp"
   },
   "source": [
    "#### Exercise 8\n",
    "To use the pre-trained embeddings, we do the following:\n",
    "\n",
    "* first, we create a tensor of size (vocab_size, embedding_size) with embedding_size = 300 and values 0 (use torch.zeros method)\n",
    "\n",
    "* we store each line in wiki.en.filtered.vec  into a list \"tokens\" whose first element is the word and the second the pretrained fasttext emebdding\n",
    "\n",
    "* If the word is in our vocabulary we set the corresponding index (use your token2int dictionary) in our 0 tensor to the corresponding fasttext embedding. \n",
    "\n",
    "**N.B.** fasttext embeddings elements needs to be converted to float so you'll need to do something like\n",
    "\n",
    "torch.FloatTensor([float(x) for x in FSEmbedding])\n",
    "\n",
    "when setting the word index in the tensor to the fasttext embeddng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4596, 300])\n"
     ]
    }
   ],
   "source": [
    "# Premier point\n",
    "token2int['<eos>'] = len(token2int) # Ajoute le token vide à notre dictionnaire\n",
    "label2int['<eos>'] = len(label2int)\n",
    "vocab_size = len(token2int)\n",
    "embeddings = torch.zeros(vocab_size, embed_size)\n",
    "\n",
    "tokens = []\n",
    "\n",
    "with open('wiki.en.filtered/wiki.en.filtered.vec') as f:\n",
    "    for line in f: # Pour chaque liste\n",
    "        token = line.split(' ')[0] # On lit le token et son embedding\n",
    "        embed = line.split(' ')[1:]\n",
    "        tokens.append((token, embed)) # On les ajoute à la liste\n",
    "        \n",
    "        if token in token2int.keys(): # Si le token est dans le vocabulaire\n",
    "            embeddings[token2int[token]] = torch.FloatTensor([float(x) for x in embed]) # On le met dans le tensor\n",
    "\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, train and evaluate your neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise  9 - Define your neural network (TODO: Provide missing values indicated by ??)\n",
    "\n",
    "As in the preceding Exercise sheet on neural classification, we define our RNN network as a subclass of pytorch RNN module. \n",
    "\n",
    "Our RNN consist of three layers:\n",
    "* the embedding layer: wich maps each token in the input to its fasttext embedding\n",
    "* A GRU layer: the recurrent layers\n",
    "* A decision layer which maps each input token to a label\n",
    "\n",
    "##### Padding\n",
    "If the input sentence is shorter than the maximum length, the remaining positions are filled with 0, the integer associated with the padding symbol. To exclude padding symbols  from the learning process (they are uninformative), include the padding_idx=vocab['<eos>'] option in the definition of the embedding layer and the \n",
    "\"bias=False\" option in the definition of the GRU layer. This forces the GRU hidden state to be null for all padding symbols. \n",
    "    \n",
    "##### Pre-trained Embeddings\n",
    "To ensure that the pretrained word embeddings are used:\n",
    "* set the `weight` attribute of the embedding layer to the pretrained embeddings\n",
    "* Use `requires_grad=False` to freeze the embedding layer so that the fasttext embeddings are not modified during learning.   \n",
    "If you do not use this option the embeddings are fine tuned during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "bbrhekbVbKN4",
    "outputId": "5fcdb589-e8f2-4a53-e113-541f9186a5cc"
   },
   "outputs": [],
   "source": [
    "# Les ligne à compléter étaient \"self.embed = nn.Embedding(???, ???, padding_idx=token2int['<eos>'])\"\n",
    "# Et output, hidden = self.rnn(???)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Le premier paramètre pour une couche Embedding doit être la taille du vocabulaire\n",
    "        # Et son deuxième paramètre doit être la dimension des embeddings\n",
    "        # Ici le premier doit donc être vocab_size et le deuxième embed_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)#, padding_idx=token2int['<eos>'])\n",
    "        self.embed.weight = nn.Parameter(embeddings, requires_grad=False)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, bias=False, num_layers=1, bidirectional=False, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.decision = nn.Linear(hidden_size * 1 * 1, len(label2int))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embed = self.embed(x)\n",
    "        output, hidden = self.rnn(embed) # On veut appeler notre réseau de neurones sur la sortie de la couche précédente\n",
    "        # c-a-d les embeddings qu'on vient de créer à la ligne du dessus\n",
    "        return self.decision(self.dropout(output))\n",
    "\n",
    "rnn_model = RNN()\n",
    "rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYepFTvcbKOF"
   },
   "source": [
    "#### Define a function to evaluate the performance of the model (PROVIDED)\n",
    "\n",
    "* the CrossEntropyLoss takes as input 2D matrices of shape (batch_size * sequence_length, num_labels)\n",
    "* Scores shape is adjusted accordingly\n",
    "* References are reshaped to (batch_size * sequence_length).\n",
    "* the max used to compute predictions applies to the last dimension of the y_scores tensors\n",
    "* To ignore padding symbols when computing the score, we create a matrix \"mask\" which contains 1 for all non nul elements of the Y matrix and O otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Bc_mfttznImx",
    "outputId": "67ea45dd-650c-4ee7-a19a-4adcf6cd1988"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.02226072406768799, 0.4908746556473829)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perf(model, loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    total_loss = correct = num_loss = num_perf = 0\n",
    "    for x, y in loader:\n",
    "      with torch.no_grad():\n",
    "        y_scores = model(x)\n",
    "        loss = criterion(y_scores.view(y.size(0) * y.size(1), -1), y.view(y.size(0) * y.size(1)))\n",
    "        y_pred = torch.max(y_scores, 2)[1]\n",
    "        mask = (y != 0)\n",
    "        correct += torch.sum((y_pred.data == y) * mask)\n",
    "        total_loss += loss.item()\n",
    "        num_loss += len(y)\n",
    "        num_perf += torch.sum(mask).item()\n",
    "    return total_loss / num_loss, correct.item() / num_perf\n",
    "\n",
    "perf(rnn_model, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MbPWKZO2bKOJ"
   },
   "source": [
    "#### Exercise 10 - Define the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copié du TD précédent\n",
    "\n",
    "def fit(model, epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_samples = 0\n",
    "        \n",
    "        for x_data, y_data in train_loader:\n",
    "            x_data = x_data.to(device)\n",
    "            y_data = y_data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_scores = model(x_data)\n",
    "            loss = criterion(y_scores.transpose(1, 2), y_data) # Modifications faites ici\n",
    "            num_samples += len(y_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        valid_loss, valid_acc = perf(model, valid_loader)\n",
    "        print(f'Epoch {epoch + 1}/{epochs} | Train loss: {total_loss/num_samples:.4f} | Valid loss: {valid_loss:.4f} | Acc: {valid_acc:.4%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 | Train loss: 0.0085 | Valid loss: 0.0024 | Acc: 94.5764%\n",
      "Epoch 2/25 | Train loss: 0.0016 | Valid loss: 0.0014 | Acc: 97.7445%\n",
      "Epoch 3/25 | Train loss: 0.0011 | Valid loss: 0.0012 | Acc: 98.0544%\n",
      "Epoch 4/25 | Train loss: 0.0009 | Valid loss: 0.0011 | Acc: 97.7961%\n",
      "Epoch 5/25 | Train loss: 0.0007 | Valid loss: 0.0010 | Acc: 98.3471%\n",
      "Epoch 6/25 | Train loss: 0.0007 | Valid loss: 0.0010 | Acc: 98.0372%\n",
      "Epoch 7/25 | Train loss: 0.0006 | Valid loss: 0.0010 | Acc: 98.0716%\n",
      "Epoch 8/25 | Train loss: 0.0006 | Valid loss: 0.0010 | Acc: 98.2782%\n",
      "Epoch 9/25 | Train loss: 0.0005 | Valid loss: 0.0009 | Acc: 98.2955%\n",
      "Epoch 10/25 | Train loss: 0.0005 | Valid loss: 0.0010 | Acc: 98.2955%\n",
      "Epoch 11/25 | Train loss: 0.0005 | Valid loss: 0.0010 | Acc: 98.3988%\n",
      "Epoch 12/25 | Train loss: 0.0004 | Valid loss: 0.0009 | Acc: 98.5709%\n",
      "Epoch 13/25 | Train loss: 0.0004 | Valid loss: 0.0009 | Acc: 98.5537%\n",
      "Epoch 14/25 | Train loss: 0.0004 | Valid loss: 0.0010 | Acc: 98.1921%\n",
      "Epoch 15/25 | Train loss: 0.0003 | Valid loss: 0.0009 | Acc: 98.4848%\n",
      "Epoch 16/25 | Train loss: 0.0003 | Valid loss: 0.0010 | Acc: 98.4160%\n",
      "Epoch 17/25 | Train loss: 0.0003 | Valid loss: 0.0010 | Acc: 98.5882%\n",
      "Epoch 18/25 | Train loss: 0.0003 | Valid loss: 0.0010 | Acc: 98.4332%\n",
      "Epoch 19/25 | Train loss: 0.0002 | Valid loss: 0.0010 | Acc: 98.3471%\n",
      "Epoch 20/25 | Train loss: 0.0002 | Valid loss: 0.0011 | Acc: 98.3988%\n",
      "Epoch 21/25 | Train loss: 0.0002 | Valid loss: 0.0011 | Acc: 98.3299%\n",
      "Epoch 22/25 | Train loss: 0.0002 | Valid loss: 0.0011 | Acc: 98.4332%\n",
      "Epoch 23/25 | Train loss: 0.0002 | Valid loss: 0.0012 | Acc: 98.3127%\n",
      "Epoch 24/25 | Train loss: 0.0001 | Valid loss: 0.0012 | Acc: 98.4848%\n",
      "Epoch 25/25 | Train loss: 0.0001 | Valid loss: 0.0012 | Acc: 98.2782%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "fit(rnn_model, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply your model to a sentence\n",
    "\n",
    "Accuracy scores might be deceiving. We also need to look at the predictions on some example sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 11 Apply your model to a sentence\n",
    "\n",
    "We define a `tag_sentence` function which:\n",
    "\n",
    "* takes a input the learned model and a sentence identifier i\n",
    "* retrieves from the data tensor X_valid the tensor for the i-th sentence (call it \"sentence\")\n",
    "* retrieves from the data tensor Y_valid the tensor of labels for the i-th sentence \n",
    "* put the model into evaluation mode\n",
    "* execute the model on the sentence tensor (\"sentence\")\n",
    "* extract the top predictions (use argmax)\n",
    "* print out the list of predicted tags \n",
    "   - use t.item() to get a value out of a tensor\n",
    "   - use your dictionary int2label to print out the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sentence(model, i):\n",
    "    int2label[3] = 'N/A'\n",
    "    sentence = X_valid[i]\n",
    "    labels = Y_valid[i]\n",
    "    model.eval()\n",
    "    y_scores = model(sentence)\n",
    "    y_pred = y_scores.argmax(1) # On choisit la classe avec les scores les plus élevés\n",
    "    print('TOKEN'.ljust(10), 'PRED'.ljust(5), 'TRUE')\n",
    "    print('-'*20)\n",
    "    for j, pred in enumerate(y_pred):\n",
    "        print(\n",
    "              int2token[sentence[j].item()].ljust(10),\n",
    "              int2label[pred.item()].ljust(5),\n",
    "              int2label[labels[j].item()]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN      PRED  TRUE\n",
      "--------------------\n",
      "some       B     B\n",
      "people     I     I\n",
      "stare      O     O\n",
      "into       O     O\n",
      "the        B     B\n",
      "distance   I     I\n",
      "as         O     O\n",
      "a          B     B\n",
      "barber     I     I\n",
      "gives      O     O\n",
      "an         B     B\n",
      "asian      I     I\n",
      "man        I     I\n",
      "a          B     B\n",
      "haircut    I     I\n",
      ".          O     O\n"
     ]
    }
   ],
   "source": [
    "tag_sentence(rnn_model, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'int2label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mint2label\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'int2label' is not defined"
     ]
    }
   ],
   "source": [
    "int2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "05_tagging-crf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
