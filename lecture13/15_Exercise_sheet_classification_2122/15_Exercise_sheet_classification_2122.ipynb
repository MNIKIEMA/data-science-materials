{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise \"Lecture 13: Classification\"\n",
    "\n",
    "\n",
    "In this set of exercises, we will use classification to classify Wikipedia articles into 16 categories. \n",
    "\n",
    "\n",
    "The exercises cover the following points:\n",
    "\n",
    "* Storing the data into an pandas dataframe and inspecting the data\n",
    "* Converting the corpus into a tfd-idf document token matrix\n",
    "* Learning a perceptron model from the data \n",
    "* Inspecting the results\n",
    "\n",
    "Data: wkp_sorted.zip      \n",
    "\n",
    "Python libraries\n",
    "- sklearn\n",
    "- pandas \n",
    "- nltk\n",
    "- numpy\n",
    "\n",
    "Cheat sheets\n",
    "- classification_cheat_sheet.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 1**\n",
    "\n",
    "Create a pandas dataframe containing the news data\n",
    "\n",
    "* The data file is in \"wkp_sorted.zip\"\n",
    "* Use the [load_files](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html) method from sklearn.datasets to load all files \n",
    "* load_files returns a dictionnary named \"data\" with keys \"data\" (a list of strings, one string per file), \"target\" (a list of label indexes, one label index per file) and \"target_names\" (the list of categories). \n",
    "* Use [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) method to create a dataframe whose headers are \"texts\" and \"label_idx\". Each row aligns the content of a file (\"text\" column) and its corresponding label index (the \"label_idx\" column). \n",
    "* Use  \"target_names\" to create a dictionary idx_to_label mapping the label indices (0 to 15) to the corresponding Wikipedia categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/experiments/cours nlp/data science/lecture13/')\n",
    "d = load_files(\"wkp_sorted/\", encoding = \"utf8\")\n",
    "df = pd.DataFrame(zip(d['data'], d['target'], d['filenames']),  columns=['Text','Target', 'Filenames'])\n",
    "df.Filenames = df.Filenames.apply(os.path.basename)\n",
    "df['Target_name'] = df.Target.apply(lambda x : d['target_names'][x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "- Shuffle the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop(columns=\"Filenames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Target</th>\n",
       "      <th>Target_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Hubble search for transition comets (Transitio...</td>\n",
       "      <td>3</td>\n",
       "      <td>Astronomical_objects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Carniny Amateur &amp; Youth FC is a junior-level f...</td>\n",
       "      <td>11</td>\n",
       "      <td>Sports_teams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Joe Petagno (born January 1, 1948) is an Ameri...</td>\n",
       "      <td>1</td>\n",
       "      <td>Artists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Al-Tilmiz (Arabic: التلميذ, 'The Pupil') was a...</td>\n",
       "      <td>15</td>\n",
       "      <td>Written_communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>An airport bus, or airport shuttle bus or airp...</td>\n",
       "      <td>0</td>\n",
       "      <td>Airports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ajman International Airport (Arabic: مطار عجما...</td>\n",
       "      <td>0</td>\n",
       "      <td>Airports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Al Anwa Aviation is a charter airline based ou...</td>\n",
       "      <td>13</td>\n",
       "      <td>Transport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>A carrozza, also referred to as mozzarella in ...</td>\n",
       "      <td>8</td>\n",
       "      <td>Foods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Gre-No-Li is a contraction of the surnames of ...</td>\n",
       "      <td>12</td>\n",
       "      <td>Sportspeople</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Savonlinna (UK: , Finnish: [ˈsɑʋonˌlinːɑ], lit...</td>\n",
       "      <td>5</td>\n",
       "      <td>City</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Target  \\\n",
       "134  Hubble search for transition comets (Transitio...       3   \n",
       "44   Carniny Amateur & Youth FC is a junior-level f...      11   \n",
       "37   Joe Petagno (born January 1, 1948) is an Ameri...       1   \n",
       "65   Al-Tilmiz (Arabic: التلميذ, 'The Pupil') was a...      15   \n",
       "111  An airport bus, or airport shuttle bus or airp...       0   \n",
       "..                                                 ...     ...   \n",
       "3    Ajman International Airport (Arabic: مطار عجما...       0   \n",
       "30   Al Anwa Aviation is a charter airline based ou...      13   \n",
       "60   A carrozza, also referred to as mozzarella in ...       8   \n",
       "98   Gre-No-Li is a contraction of the surnames of ...      12   \n",
       "50   Savonlinna (UK: , Finnish: [ˈsɑʋonˌlinːɑ], lit...       5   \n",
       "\n",
       "               Target_name  \n",
       "134   Astronomical_objects  \n",
       "44            Sports_teams  \n",
       "37                 Artists  \n",
       "65   Written_communication  \n",
       "111               Airports  \n",
       "..                     ...  \n",
       "3                 Airports  \n",
       "30               Transport  \n",
       "60                   Foods  \n",
       "98            Sportspeople  \n",
       "50                    City  \n",
       "\n",
       "[160 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the input texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**\n",
    "\n",
    "* Extract $X$ and $Y$ from the dataframe \n",
    "* $X$ = the features used for clustering. The features of a news items is the list of tokens contained in that item. We hope that words can help classify news items into the correct category\n",
    "* $Y$ = the category (Astronaut, etc.) of each Wikipedia article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1['Text']\n",
    "y = df1.Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**\n",
    "\n",
    "Create train and test data\n",
    "* Use sklearn [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**\n",
    "\n",
    "Vectorize the input (X)\n",
    "\n",
    "Use sklearn [TfidfVectorizer]( https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) method to turn the news items into a TF-IDF matrix where each row represents a news item, the columns are tokens and the cell contains the tf-idf score of each token.\n",
    "\n",
    "* Import the [TfidfVectorize](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) method from sklearn\n",
    "* Create a tf-idf vectorizer. The maximum nb of features should be set to 400. Set use_idf to True, stop_words to \"english\" and the tokenizer to nltk.word_tokenize.\n",
    "* Apply the tfidf_vectorizer.fit_transform method to X to vectorize all input texts (i.e., both X_train and X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=400,\n",
    "                                   use_idf=True,\n",
    "                                   stop_words='english',\n",
    "                                   tokenizer=nltk.word_tokenize,\n",
    "                                   ngram_range=(1, 3))\n",
    "X_train_vec = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_vec = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6**\n",
    "\n",
    "- Use the [get_feature_names](https://scikit-learn.org/stable/modules/feature_extraction.html) method to print out the features\n",
    "- Look at the features: are they all useful or would further preprocessing help eliminate uniformative tokens ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "features = tfidf_vectorizer.get_feature_names_out()\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a perceptron classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise 7**\n",
    "\n",
    "* Import the [Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html) module \n",
    "* Create an object of the class Perceptron\n",
    "* Train the model using the [fit](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron.fit) method\n",
    "* Test the model using the [predict](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron.fit)\n",
    " method\n",
    "* Print out expected values and predictions\n",
    "* Print out accuracy using [sklearn accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) method\n",
    "* Print out [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)and [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "perceptron_clf = Perceptron()\n",
    "perceptron_clf.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = perceptron_clf.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14 15  7 12 14] [13, 2, 7, 5, 7]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[:5], y_train[:5].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.53125\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8**\n",
    "\n",
    "* sklearn tfidf_vectorizer creates a vocabulary dictionary {(k,v),} where k is a token and v is an index (integer)\n",
    "   - Create a dictionary ix_to_tag mapping each index to the corresponding token  and a dictionary tag_to_idx mapping each token to the corresponding index\n",
    "* The [coef_ ](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron.fit) attribute contains the learned weights for each feature. Size = nb of classes, nb of features. \n",
    "* Save the feature weights in a dictionary where key = token index, value = weight\n",
    "* Define a function that derives a sorted list of (tokenIndex, weight) pairs\n",
    "* For each class, \n",
    "   -  get the feature weights for each class\n",
    "   - Sort the weights\n",
    "   - Print out the first 6 token:weight pairs (replace token indices by the corresponding token)\n",
    "\n",
    "To better see whether the top words of each class match the corresponding class, use the \"idx_to_label\" dictionary defined in Exercise 1 to rewrite each class idx to the corresponding label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "ix_to_tokens = { v:k for k,v in vocab.items() }\n",
    "tag_to_ix = { v:k for k,v in ix_to_tokens.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_weight = {k:v for k,v in zip(ix_to_tokens.keys(), perceptron_clf.coef_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def srt_coef():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Performing grid search to find the best possible score and best alpha value (PROVIDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Tuning using grid search cross-validation\n",
    "# Create an object GridSearchCV\n",
    "\n",
    "parameters = [a for a in np.linspace(0.01,1,11)]\n",
    "clf = GridSearchCV( estimator=MultinomialNB(), \n",
    "                   param_grid={'alpha':parameters},\n",
    "                   scoring='accuracy',\n",
    "                   return_train_score=True,\n",
    "                   cv=5\n",
    "                  )\n",
    "\n",
    "# Start the search over the hyper-parameters by calling the fit function over X_train\n",
    "clf.fit( X_train_tf, Y_train )\n",
    "\n",
    "# Print the results of the CV using the attribute *cv_results_*\n",
    "cv_res = pd.DataFrame(clf.cv_results_)\n",
    "cv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out the best score\n",
    "print(\"Best score: %0.3f\" % clf.best_score_)\n",
    "\n",
    "# Printing out the \n",
    "best_parameters = clf.best_estimator_.get_params()\n",
    "print(\"Best alpha\", best_parameters['alpha'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
