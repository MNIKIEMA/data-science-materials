{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading files from a directory into a panda dataframe**\n",
    "\n",
    "* the  [load_files](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html) takes as input a directory in which the immediate subdirectories are category names for the text files they contain \n",
    "\n",
    "```\n",
    "DIR/\n",
    " category_1/\n",
    "    file_1.txt file_2.txt … file_42.txt\n",
    " category_2/\n",
    "    file_43.txt file_44.txt …\n",
    "```\n",
    "\n",
    "* the load_files method\n",
    "recursively uploads all files in a directory and return a dictionary object with attributes \"data\", the text content of the input files and \"target_names\", the names of the subdirectory containing the text files. \n",
    "* the code below use this method to extract the content and categories of the text files contained in the ../data/bbc/ directory and to store them into a pandas frame with headers 'text' and 'label' respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../data/bbc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_files\n",
    "# Loading all files in \"dir\" directory into a pandas dataframe\n",
    "DATA_DIR = \"../data/bbc/\"\n",
    "data = load_files(DATA_DIR, encoding=\"utf-8\", decode_error=\"replace\")\n",
    "df = pd.DataFrame(list(zip(data['data'], data['target_names'])), columns=['text', 'label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting a corpus of texts into a tf-idf matrix**\n",
    "* the input is our corpus, a list of texts\n",
    "* we can specify how the text is tokenised and whether stop-words are removed\n",
    "* the output is a matrix where each row is a text and each column is a token. The cells of the matrix contain the tf-idf score of the token in that text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "corpus = ['Apples and pears are fruit','A pear is a fruit','dogs and cats are animal','a cat is an animal']\n",
    "\n",
    "from nltk import word_tokenize\n",
    "# Create a TFIDF vectorizer to convert convert words to vectors\n",
    "vectorizer = TfidfVectorizer(max_features=10,\n",
    "                                       use_idf=True,\n",
    "                                       stop_words='english',\n",
    "                                       tokenizer=nltk.word_tokenize)\n",
    "# Apply the vectorizer to the input texts\n",
    "M = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output matrix contains 4 rows, one for each input document \n",
    "# and 5 columns as we set the max nb of features to 5\n",
    "print(M.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Viewing the features used by the clustering algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering** \n",
    "\n",
    "[KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "* we just input the tf-idf matrix (the representation of the input texts) to a KMeans clustering model\n",
    "* the \"fit\" method fits the data i.e., aims to find the best set of clusters for it\n",
    "* n_init: the number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Create a KMeans clustering model\n",
    "km = KMeans(n_clusters=2, init='k-means++', max_iter=300, n_init=5, verbose=0, random_state=3425)\n",
    "# Apply the clustering model on the tf-idf matrix (the features)\n",
    "km.fit(M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Printing out clustering results**\n",
    "* You can view which item belongs to which cluster using the labels_ attribute\n",
    "* If you want to use the predicted cluster labels eg for viewing (to compare with the ground truth labels) you need to explicitely store these into a list as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the predicted labels\n",
    "predicted_labels = km.labels_\n",
    "# Store the predicted clusters into a list\n",
    "predicted_labels = predicted_labels.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing clustering evaluation metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Show ground truth labels (if available)\n",
    "labels = [\"fruit\",\"fruit\",\"animals\",\"animals\"]\n",
    "print( labels)\n",
    "# Show predicted labels\n",
    "print( km.labels_)\n",
    "# Compute and show evaluation scores\n",
    "# When a ground truth is available\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "# When no ground truth is available\n",
    "#print(\"Silhouette Coefficient: %0.3f\"\n",
    "#      % metrics.silhouette_score(tfidf_matrix, km.labels_, sample_size=1000))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Printing out the number of items per clusters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First store documents, labels and cluster labels into a Pandas datafram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'text':corpus,'label':labels,'cluster':km.labels_}\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then count the number of each occurrence in the cluster column (= the number of documents for each cluster label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the top tokens of each cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(\"Top terms per cluster:\")\n",
    "# get the number of clusters\n",
    "true_k = np.unique(labels).shape[0]\n",
    "\n",
    "# get the cluster center of each cluster \n",
    "# argsort() return the index of each dimension in the cluster center and sort them in increasing value order\n",
    "# [:, ::-1] reverts the argsort() list to place the indices with highest value first (decreasing order)\n",
    "order_centroids = km.cluster_centers_.argsort()[:]\n",
    "\n",
    "# terms maps a vectorizer index to the corresponding token\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# for each cluster\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    # print out the token of the centroid (order by decreasing tf-idf value)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
